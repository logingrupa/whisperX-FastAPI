---
phase: 02-file-upload-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - app/api/streaming_upload_api.py
  - app/infrastructure/storage/__init__.py
  - app/infrastructure/storage/streaming_target.py
  - app/core/upload_config.py

autonomous: true

must_haves:
  truths:
    - "500MB+ file uploads do not cause server memory spikes"
    - "Uploads stream directly to disk as chunks arrive"
    - "Files exceeding 5GB are rejected during upload (not after)"
  artifacts:
    - path: "app/api/streaming_upload_api.py"
      provides: "POST /upload/stream endpoint for large file uploads"
      exports: ["streaming_upload_router"]
    - path: "app/infrastructure/storage/streaming_target.py"
      provides: "Custom FileTarget for streaming-form-data"
      exports: ["StreamingFileTarget"]
    - path: "app/core/upload_config.py"
      provides: "Upload configuration constants"
      exports: ["UPLOAD_DIR", "MAX_FILE_SIZE", "CHUNK_SIZE"]
  key_links:
    - from: "app/api/streaming_upload_api.py"
      to: "streaming-form-data"
      via: "StreamingFormDataParser"
      pattern: "StreamingFormDataParser.*data_received"
    - from: "app/api/streaming_upload_api.py"
      to: "app/infrastructure/storage/streaming_target.py"
      via: "StreamingFileTarget import"
      pattern: "from app.infrastructure.storage.streaming_target import"
---

<objective>
Implement streaming upload infrastructure that handles large audio/video files (up to 5GB) without memory exhaustion.

Purpose: Current `FileService.save_upload()` reads entire file into memory with `file.file.read()`, which will crash the server on large files. This plan adds true streaming using `streaming-form-data` library to write directly to disk as chunks arrive.

Output: New streaming upload endpoint at POST /upload/stream that can handle 5GB files with constant memory usage (~10MB regardless of file size).
</objective>

<execution_context>
@C:\Users\rolan\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\rolan\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-file-upload-infrastructure/02-RESEARCH.md

# Existing code to understand patterns
@app/api/audio_api.py
@app/services/file_service.py
@app/core/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add streaming upload dependencies</name>
  <files>pyproject.toml</files>
  <action>
Add the following dependencies to the `dependencies` list in pyproject.toml:

```
"streaming-form-data>=1.19.0",
"aiofiles>=25.1.0",
"puremagic>=1.30",
```

These are the research-verified libraries for:
- streaming-form-data: Cython-based multipart parser that streams directly to disk
- aiofiles: Async file I/O to avoid blocking event loop
- puremagic: Pure Python magic byte detection (no libmagic dependency)

After editing, run `uv sync` to install the new dependencies.

Note: Do NOT add APScheduler yet - cleanup scheduler will be addressed in a future plan once the basic upload flow works.
  </action>
  <verify>Run `uv sync` successfully, then verify imports work: `python -c "import streaming_form_data; import aiofiles; import puremagic; print('OK')"`</verify>
  <done>New dependencies installed and importable</done>
</task>

<task type="auto">
  <name>Task 2: Create upload configuration module</name>
  <files>app/core/upload_config.py</files>
  <action>
Create a new configuration module for upload-related settings.

```python
"""Configuration for file upload handling."""

from pathlib import Path
from tempfile import gettempdir

# Upload directory - use system temp by default
UPLOAD_DIR = Path(gettempdir()) / "whisperx_uploads"

# Maximum file size: 5GB per CONTEXT.md
MAX_FILE_SIZE = 5 * 1024 * 1024 * 1024  # 5GB in bytes

# Chunk size for streaming - 1MB is standard per RESEARCH.md
CHUNK_SIZE = 1024 * 1024  # 1MB

# Partial upload expiry - 10 minutes per CONTEXT.md
PARTIAL_UPLOAD_EXPIRY_SECONDS = 600

# Allowed extensions for streaming uploads (must match Config.ALLOWED_EXTENSIONS)
# Using explicit set here to avoid circular imports with Config
ALLOWED_UPLOAD_EXTENSIONS = {
    ".mp3", ".wav", ".awb", ".aac", ".ogg", ".oga", ".m4a", ".wma", ".amr",  # audio
    ".mp4", ".mov", ".avi", ".wmv", ".mkv",  # video
    ".flac", ".webm",  # additional from CONTEXT.md
}
```

This module centralizes upload configuration separate from the main Config to:
1. Avoid circular imports
2. Make upload settings easy to find and modify
3. Follow SRP - upload config in upload config module
  </action>
  <verify>Run `python -c "from app.core.upload_config import UPLOAD_DIR, MAX_FILE_SIZE, CHUNK_SIZE; print(f'Upload dir: {UPLOAD_DIR}, Max size: {MAX_FILE_SIZE / (1024**3):.1f}GB')"`</verify>
  <done>Upload configuration module exists with documented constants</done>
</task>

<task type="auto">
  <name>Task 3: Create streaming file target and upload endpoint</name>
  <files>
    app/infrastructure/storage/__init__.py
    app/infrastructure/storage/streaming_target.py
    app/api/streaming_upload_api.py
    app/main.py
  </files>
  <action>
**Step 1: Create storage package init**

Create `app/infrastructure/storage/__init__.py`:
```python
"""Storage infrastructure for file uploads."""

from app.infrastructure.storage.streaming_target import StreamingFileTarget

__all__ = ["StreamingFileTarget"]
```

**Step 2: Create StreamingFileTarget**

Create `app/infrastructure/storage/streaming_target.py`:
```python
"""Custom streaming target for large file uploads."""

from pathlib import Path
from typing import Optional

from streaming_form_data.targets import BaseTarget
from streaming_form_data.validators import MaxSizeValidator

from app.core.logging import logger
from app.core.upload_config import MAX_FILE_SIZE


class StreamingFileTarget(BaseTarget):
    """
    Custom target that streams uploaded file directly to disk.

    Uses streaming-form-data's BaseTarget interface to receive chunks
    as they arrive from the multipart parser, writing directly to disk
    without buffering the entire file in memory.
    """

    def __init__(self, filepath: Path, max_size: int = MAX_FILE_SIZE) -> None:
        """
        Initialize the streaming file target.

        Args:
            filepath: Path where the file will be written
            max_size: Maximum allowed file size in bytes (default: 5GB)
        """
        super().__init__(validator=MaxSizeValidator(max_size))
        self.filepath = filepath
        self._file: Optional[object] = None
        self._bytes_written = 0

    def on_start(self) -> None:
        """Called when file upload starts. Opens the file handle."""
        self.filepath.parent.mkdir(parents=True, exist_ok=True)
        self._file = open(self.filepath, "wb")
        self._bytes_written = 0
        logger.debug("Started streaming upload to: %s", self.filepath)

    def on_data_received(self, chunk: bytes) -> None:
        """
        Called for each chunk received.

        Args:
            chunk: Bytes received from the upload stream
        """
        if self._file is not None:
            self._file.write(chunk)  # type: ignore[union-attr]
            self._bytes_written += len(chunk)

    def on_finish(self) -> None:
        """Called when upload completes. Closes the file handle."""
        if self._file is not None:
            self._file.close()  # type: ignore[union-attr]
            self._file = None
        logger.info(
            "Completed streaming upload: %s (%d bytes)",
            self.filepath,
            self._bytes_written,
        )

    @property
    def bytes_written(self) -> int:
        """Return total bytes written to file."""
        return self._bytes_written
```

**Step 3: Create streaming upload API**

Create `app/api/streaming_upload_api.py`:
```python
"""Streaming upload endpoint for large audio/video files."""

import uuid
from pathlib import Path

from fastapi import APIRouter, HTTPException, Request, status
from streaming_form_data import StreamingFormDataParser
from streaming_form_data.targets import ValueTarget
from streaming_form_data.validators import ValidationError

from app.core.logging import logger
from app.core.upload_config import (
    ALLOWED_UPLOAD_EXTENSIONS,
    MAX_FILE_SIZE,
    UPLOAD_DIR,
)
from app.infrastructure.storage.streaming_target import StreamingFileTarget

streaming_upload_router = APIRouter(prefix="/upload", tags=["Upload"])


@streaming_upload_router.post("/stream")
async def streaming_upload(request: Request) -> dict[str, str | int]:
    """
    Stream large file upload directly to disk.

    This endpoint handles files up to 5GB without loading them into memory.
    The file is streamed directly to disk as chunks arrive.

    Args:
        request: FastAPI Request object for accessing raw stream

    Returns:
        dict with upload_id, filename, and size_bytes

    Raises:
        HTTPException: 400 for invalid content-type or file format
        HTTPException: 413 for files exceeding 5GB limit
    """
    content_type = request.headers.get("content-type", "")
    if "multipart/form-data" not in content_type:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Content-Type must be multipart/form-data",
        )

    # Generate unique upload ID
    upload_id = str(uuid.uuid4())
    temp_path = UPLOAD_DIR / f"{upload_id}.tmp"

    # Ensure upload directory exists
    UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

    # Create streaming parser
    parser = StreamingFormDataParser(headers={"Content-Type": content_type})

    # Register targets for file and optional filename field
    file_target = StreamingFileTarget(temp_path)
    filename_target = ValueTarget()

    parser.register("file", file_target)
    parser.register("filename", filename_target)

    try:
        # Stream chunks directly to parser (and thus to disk)
        async for chunk in request.stream():
            parser.data_received(chunk)

    except ValidationError:
        # Size limit exceeded - clean up partial file
        temp_path.unlink(missing_ok=True)
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail=f"File too large. Maximum size: {MAX_FILE_SIZE / (1024**3):.1f}GB",
        )

    # Get filename from multipart or fallback
    original_filename = file_target.multipart_filename
    if not original_filename:
        # Try the separate filename field
        filename_value = filename_target.value
        if filename_value:
            original_filename = filename_value.decode("utf-8")
        else:
            original_filename = f"{upload_id}.bin"

    # Validate extension
    extension = Path(original_filename).suffix.lower()
    if extension not in ALLOWED_UPLOAD_EXTENSIONS:
        # Clean up the uploaded file
        temp_path.unlink(missing_ok=True)
        allowed_list = ", ".join(sorted(ALLOWED_UPLOAD_EXTENSIONS))
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unsupported file format: {extension}. Allowed: {allowed_list}",
        )

    # Rename temp file with proper extension
    final_path = UPLOAD_DIR / f"{upload_id}{extension}"
    temp_path.rename(final_path)

    logger.info(
        "Streaming upload complete: %s -> %s (%d bytes)",
        original_filename,
        final_path,
        file_target.bytes_written,
    )

    return {
        "upload_id": upload_id,
        "filename": original_filename,
        "size_bytes": file_target.bytes_written,
        "path": str(final_path),
    }
```

**Step 4: Register router in main.py**

Add the streaming upload router to main.py. Find the section where other routers are registered (look for `app.include_router`) and add:

```python
from app.api.streaming_upload_api import streaming_upload_router

# Add with other router registrations:
app.include_router(streaming_upload_router)
```
  </action>
  <verify>
1. Start server: `uv run uvicorn app.main:app --reload`
2. Test with curl using a small file first:
   ```bash
   curl -X POST http://localhost:8000/upload/stream \
     -F "file=@test_audio.mp3" \
     -w "\n%{http_code}\n"
   ```
3. Should return JSON with upload_id, filename, size_bytes, path and HTTP 200
4. Verify the file exists in the temp upload directory
  </verify>
  <done>
    - StreamingFileTarget writes chunks directly to disk
    - POST /upload/stream endpoint accepts multipart uploads
    - Files over 5GB rejected with 413
    - Invalid extensions rejected with 400
    - Router registered in main.py
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Dependency check:**
   ```bash
   python -c "import streaming_form_data; import aiofiles; import puremagic; print('Dependencies OK')"
   ```

2. **Small file upload test:**
   ```bash
   # Create test file
   echo "test content" > /tmp/test.mp3

   # Upload via streaming endpoint
   curl -X POST http://localhost:8000/upload/stream \
     -F "file=@/tmp/test.mp3" \
     -w "\n%{http_code}\n"
   ```

3. **Invalid extension rejection test:**
   ```bash
   echo "test" > /tmp/test.xyz
   curl -X POST http://localhost:8000/upload/stream \
     -F "file=@/tmp/test.xyz" \
     -w "\n%{http_code}\n"
   # Should return 400
   ```

4. **Memory usage check (manual):**
   - Monitor server memory while uploading a 100MB+ file
   - Memory should stay relatively constant (not spike by file size)
</verification>

<success_criteria>
- [ ] streaming-form-data, aiofiles, puremagic dependencies installed
- [ ] Upload config module with MAX_FILE_SIZE = 5GB
- [ ] StreamingFileTarget class streams chunks to disk
- [ ] POST /upload/stream endpoint handles multipart uploads
- [ ] Files stream directly to UPLOAD_DIR without memory buffering
- [ ] Extension validation rejects non-audio/video files
- [ ] Size validation rejects files over 5GB during upload
</success_criteria>

<output>
After completion, create `.planning/phases/02-file-upload-infrastructure/02-01-SUMMARY.md`
</output>
